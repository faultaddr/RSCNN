train_cls.py:37: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(f)

**************************

[workers]: 6

[num_points]: 1024

[num_classes]: 40

[batch_size]: 256

[base_lr]: 0.001

[lr_clip]: 1e-05

[lr_decay]: 0.7

[decay_step]: 21

[epochs]: 500

[weight_decay]: 1e-05

[bn_momentum]: 0.9

[bnm_clip]: 0.01

[bn_decay]: 0.5

[evaluate]: 1

[val_freq_epoch]: 10

[print_freq_iter]: 40

[input_channels]: 0

[relation_prior]: 1

[checkpoint]: 

[save_path]: cls

[data_root]: /media/disk3/pyy/RSCNN_Pytorch1.0/modelnet40_ply_hdf5_2048

**************************

/media/disk3/pyy/RSCNN_Pytorch1.0/modelnet40_ply_hdf5_2048
ply_data_train0.h5
/media/disk3/pyy/RSCNN_Pytorch1.0/data/ModelNet40Loader.py:14: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.
  f = h5py.File(name)
ply_data_train1.h5
ply_data_train2.h5
ply_data_train3.h5
ply_data_train4.h5
39
/media/disk3/pyy/RSCNN_Pytorch1.0/modelnet40_ply_hdf5_2048
ply_data_test0.h5
ply_data_test1.h5
39
[epoch   1:   0/ 38] 	 train loss: 3.895471 	 lr: 0.00100
[epoch   2:   0/ 38] 	 train loss: 2.815204 	 lr: 0.00100
[epoch   3:   0/ 38] 	 train loss: 2.372798 	 lr: 0.00100
[epoch   4:   0/ 38] 	 train loss: 2.179212 	 lr: 0.00100
[epoch   5:   0/ 38] 	 train loss: 2.049825 	 lr: 0.00100
[epoch   6:   0/ 38] 	 train loss: 2.052462 	 lr: 0.00100
[epoch   7:   0/ 38] 	 train loss: 1.959932 	 lr: 0.00100
[epoch   8:   0/ 38] 	 train loss: 1.963830 	 lr: 0.00100
[epoch   9:   0/ 38] 	 train loss: 1.730602 	 lr: 0.00100
[epoch  10:   0/ 38] 	 train loss: 1.803213 	 lr: 0.00100
now evaluate...

val loss: 2.445857 	 acc: 0.285251

[epoch  11:   0/ 38] 	 train loss: 1.621432 	 lr: 0.00100
[epoch  12:   0/ 38] 	 train loss: 1.601418 	 lr: 0.00100
[epoch  13:   0/ 38] 	 train loss: 1.584861 	 lr: 0.00100
[epoch  14:   0/ 38] 	 train loss: 1.373205 	 lr: 0.00100
[epoch  15:   0/ 38] 	 train loss: 1.533995 	 lr: 0.00100
[epoch  16:   0/ 38] 	 train loss: 1.483898 	 lr: 0.00100
[epoch  17:   0/ 38] 	 train loss: 1.407494 	 lr: 0.00100
[epoch  18:   0/ 38] 	 train loss: 1.356510 	 lr: 0.00100
[epoch  19:   0/ 38] 	 train loss: 1.569494 	 lr: 0.00100
[epoch  20:   0/ 38] 	 train loss: 1.419321 	 lr: 0.00100
now evaluate...

val loss: 1.885597 	 acc: 0.445705

[epoch  21:   0/ 38] 	 train loss: 1.365304 	 lr: 0.00100
[epoch  22:   0/ 38] 	 train loss: 1.195438 	 lr: 0.00100
[epoch  23:   0/ 38] 	 train loss: 1.281144 	 lr: 0.00070
[epoch  24:   0/ 38] 	 train loss: 1.210023 	 lr: 0.00070
[epoch  25:   0/ 38] 	 train loss: 1.270963 	 lr: 0.00070
[epoch  26:   0/ 38] 	 train loss: 1.287334 	 lr: 0.00070
[epoch  27:   0/ 38] 	 train loss: 1.224983 	 lr: 0.00070
[epoch  28:   0/ 38] 	 train loss: 1.109148 	 lr: 0.00070
[epoch  29:   0/ 38] 	 train loss: 1.371460 	 lr: 0.00070
[epoch  30:   0/ 38] 	 train loss: 1.208561 	 lr: 0.00070
now evaluate...

val loss: 1.669149 	 acc: 0.492707

[epoch  31:   0/ 38] 	 train loss: 1.314897 	 lr: 0.00070
[epoch  32:   0/ 38] 	 train loss: 1.148473 	 lr: 0.00070
[epoch  33:   0/ 38] 	 train loss: 1.221597 	 lr: 0.00070
[epoch  34:   0/ 38] 	 train loss: 1.135906 	 lr: 0.00070
[epoch  35:   0/ 38] 	 train loss: 1.178101 	 lr: 0.00070
[epoch  36:   0/ 38] 	 train loss: 1.174297 	 lr: 0.00070
[epoch  37:   0/ 38] 	 train loss: 1.143248 	 lr: 0.00070
[epoch  38:   0/ 38] 	 train loss: 1.210403 	 lr: 0.00070
[epoch  39:   0/ 38] 	 train loss: 1.086517 	 lr: 0.00070
[epoch  40:   0/ 38] 	 train loss: 1.139385 	 lr: 0.00070
now evaluate...

val loss: 1.585592 	 acc: 0.525122

[epoch  41:   0/ 38] 	 train loss: 1.119428 	 lr: 0.00070
[epoch  42:   0/ 38] 	 train loss: 1.172802 	 lr: 0.00070
[epoch  43:   0/ 38] 	 train loss: 1.213666 	 lr: 0.00070
[epoch  44:   0/ 38] 	 train loss: 1.126345 	 lr: 0.00049
[epoch  45:   0/ 38] 	 train loss: 1.074587 	 lr: 0.00049
[epoch  46:   0/ 38] 	 train loss: 0.895038 	 lr: 0.00049
[epoch  47:   0/ 38] 	 train loss: 1.121997 	 lr: 0.00049
[epoch  48:   0/ 38] 	 train loss: 1.023225 	 lr: 0.00049
[epoch  49:   0/ 38] 	 train loss: 0.973345 	 lr: 0.00049
[epoch  50:   0/ 38] 	 train loss: 0.965317 	 lr: 0.00049
now evaluate...

val loss: 1.387991 	 acc: 0.580632

[epoch  51:   0/ 38] 	 train loss: 1.071321 	 lr: 0.00049
[epoch  52:   0/ 38] 	 train loss: 1.066967 	 lr: 0.00049
[epoch  53:   0/ 38] 	 train loss: 0.902833 	 lr: 0.00049
[epoch  54:   0/ 38] 	 train loss: 1.094291 	 lr: 0.00049
[epoch  55:   0/ 38] 	 train loss: 1.087865 	 lr: 0.00049
[epoch  56:   0/ 38] 	 train loss: 0.972958 	 lr: 0.00049
[epoch  57:   0/ 38] 	 train loss: 1.072445 	 lr: 0.00049
[epoch  58:   0/ 38] 	 train loss: 1.047522 	 lr: 0.00049
[epoch  59:   0/ 38] 	 train loss: 1.095249 	 lr: 0.00049
[epoch  60:   0/ 38] 	 train loss: 1.036400 	 lr: 0.00049
now evaluate...

val loss: 1.424725 	 acc: 0.566045

[epoch  61:   0/ 38] 	 train loss: 0.991468 	 lr: 0.00049
[epoch  62:   0/ 38] 	 train loss: 1.007385 	 lr: 0.00049
[epoch  63:   0/ 38] 	 train loss: 1.086258 	 lr: 0.00049
[epoch  64:   0/ 38] 	 train loss: 1.039546 	 lr: 0.00049
[epoch  65:   0/ 38] 	 train loss: 0.970209 	 lr: 0.00034
[epoch  66:   0/ 38] 	 train loss: 1.117432 	 lr: 0.00034
[epoch  67:   0/ 38] 	 train loss: 1.079453 	 lr: 0.00034
[epoch  68:   0/ 38] 	 train loss: 0.993295 	 lr: 0.00034
[epoch  69:   0/ 38] 	 train loss: 1.029750 	 lr: 0.00034
now evaluate...

val loss: 1.330863 	 acc: 0.595219

[epoch  70:   0/ 38] 	 train loss: 1.091577 	 lr: 0.00034
[epoch  71:   0/ 38] 	 train loss: 1.156491 	 lr: 0.00034
[epoch  72:   0/ 38] 	 train loss: 0.959158 	 lr: 0.00034
[epoch  73:   0/ 38] 	 train loss: 0.962204 	 lr: 0.00034
[epoch  74:   0/ 38] 	 train loss: 0.985852 	 lr: 0.00034
[epoch  75:   0/ 38] 	 train loss: 0.995609 	 lr: 0.00034
[epoch  76:   0/ 38] 	 train loss: 0.993313 	 lr: 0.00034
[epoch  77:   0/ 38] 	 train loss: 0.995398 	 lr: 0.00034
[epoch  78:   0/ 38] 	 train loss: 1.070701 	 lr: 0.00034
[epoch  79:   0/ 38] 	 train loss: 0.900736 	 lr: 0.00034
now evaluate...

val loss: 1.361759 	 acc: 0.581848

[epoch  80:   0/ 38] 	 train loss: 0.925274 	 lr: 0.00034
[epoch  81:   0/ 38] 	 train loss: 0.800471 	 lr: 0.00034
[epoch  82:   0/ 38] 	 train loss: 0.944481 	 lr: 0.00034
[epoch  83:   0/ 38] 	 train loss: 0.982273 	 lr: 0.00034
[epoch  84:   0/ 38] 	 train loss: 1.068849 	 lr: 0.00034
[epoch  85:   0/ 38] 	 train loss: 1.038087 	 lr: 0.00034
[epoch  86:   0/ 38] 	 train loss: 1.020895 	 lr: 0.00024
[epoch  87:   0/ 38] 	 train loss: 0.992401 	 lr: 0.00024
[epoch  88:   0/ 38] 	 train loss: 0.935010 	 lr: 0.00024
[epoch  89:   0/ 38] 	 train loss: 0.833979 	 lr: 0.00024
now evaluate...

val loss: 1.294979 	 acc: 0.603728

[epoch  90:   0/ 38] 	 train loss: 0.826188 	 lr: 0.00024
[epoch  91:   0/ 38] 	 train loss: 0.939151 	 lr: 0.00024
[epoch  92:   0/ 38] 	 train loss: 0.927877 	 lr: 0.00024
[epoch  93:   0/ 38] 	 train loss: 0.848345 	 lr: 0.00024
[epoch  94:   0/ 38] 	 train loss: 1.074606 	 lr: 0.00024
[epoch  95:   0/ 38] 	 train loss: 1.018564 	 lr: 0.00024
[epoch  96:   0/ 38] 	 train loss: 0.893778 	 lr: 0.00024
[epoch  97:   0/ 38] 	 train loss: 0.888765 	 lr: 0.00024
[epoch  98:   0/ 38] 	 train loss: 0.967093 	 lr: 0.00024
[epoch  99:   0/ 38] 	 train loss: 0.811212 	 lr: 0.00024
now evaluate...

val loss: 1.321361 	 acc: 0.598460

[epoch 100:   0/ 38] 	 train loss: 0.861642 	 lr: 0.00024
[epoch 101:   0/ 38] 	 train loss: 0.773291 	 lr: 0.00024
[epoch 102:   0/ 38] 	 train loss: 0.828715 	 lr: 0.00024
[epoch 103:   0/ 38] 	 train loss: 0.817710 	 lr: 0.00024
[epoch 104:   0/ 38] 	 train loss: 0.889758 	 lr: 0.00024
[epoch 105:   0/ 38] 	 train loss: 0.877594 	 lr: 0.00024
[epoch 106:   0/ 38] 	 train loss: 0.805876 	 lr: 0.00024
[epoch 107:   0/ 38] 	 train loss: 1.080537 	 lr: 0.00017
[epoch 108:   0/ 38] 	 train loss: 0.906392 	 lr: 0.00017
[epoch 109:   0/ 38] 	 train loss: 0.818699 	 lr: 0.00017
now evaluate...

val loss: 1.282820 	 acc: 0.603728

[epoch 110:   0/ 38] 	 train loss: 0.962647 	 lr: 0.00017
[epoch 111:   0/ 38] 	 train loss: 0.939655 	 lr: 0.00017
[epoch 112:   0/ 38] 	 train loss: 0.832019 	 lr: 0.00017
[epoch 113:   0/ 38] 	 train loss: 1.036070 	 lr: 0.00017
[epoch 114:   0/ 38] 	 train loss: 0.991044 	 lr: 0.00017
[epoch 115:   0/ 38] 	 train loss: 0.798174 	 lr: 0.00017
[epoch 116:   0/ 38] 	 train loss: 0.868405 	 lr: 0.00017
[epoch 117:   0/ 38] 	 train loss: 0.919151 	 lr: 0.00017
[epoch 118:   0/ 38] 	 train loss: 0.903952 	 lr: 0.00017
[epoch 119:   0/ 38] 	 train loss: 0.918983 	 lr: 0.00017
now evaluate...

val loss: 1.266427 	 acc: 0.608185

[epoch 120:   0/ 38] 	 train loss: 0.884019 	 lr: 0.00017
[epoch 121:   0/ 38] 	 train loss: 0.818366 	 lr: 0.00017
[epoch 122:   0/ 38] 	 train loss: 0.984804 	 lr: 0.00017
[epoch 123:   0/ 38] 	 train loss: 0.870447 	 lr: 0.00017
[epoch 124:   0/ 38] 	 train loss: 0.956590 	 lr: 0.00017
[epoch 125:   0/ 38] 	 train loss: 0.891402 	 lr: 0.00017
