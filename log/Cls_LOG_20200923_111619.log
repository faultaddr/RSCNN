train_cls.py:37: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.
  config = yaml.load(f)

**************************

[workers]: 6

[num_points]: 1024

[num_classes]: 40

[batch_size]: 256

[base_lr]: 0.01

[lr_clip]: 1e-05

[lr_decay]: 0.7

[decay_step]: 50

[epochs]: 5000

[weight_decay]: 1e-05

[bn_momentum]: 0.9

[bnm_clip]: 0.01

[bn_decay]: 0.5

[evaluate]: 1

[val_freq_epoch]: 10

[print_freq_iter]: 40

[input_channels]: 0

[relation_prior]: 1

[checkpoint]: 

[save_path]: cls

[data_root]: /media/disk3/pyy/RSCNN_Pytorch1.0/modelnet40_ply_hdf5_2048

**************************

/media/disk3/pyy/RSCNN_Pytorch1.0/modelnet40_ply_hdf5_2048
ply_data_train0.h5
/media/disk3/pyy/RSCNN_Pytorch1.0/data/ModelNet40Loader.py:14: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.
  f = h5py.File(name)
ply_data_train1.h5
ply_data_train2.h5
ply_data_train3.h5
ply_data_train4.h5
39
/media/disk3/pyy/RSCNN_Pytorch1.0/modelnet40_ply_hdf5_2048
ply_data_test0.h5
ply_data_test1.h5
39
[epoch   1:   0/ 38] 	 train loss: 3.895471 	 lr: 0.01000
[epoch   2:   0/ 38] 	 train loss: 2.589578 	 lr: 0.01000
[epoch   3:   0/ 38] 	 train loss: 2.179967 	 lr: 0.01000
[epoch   4:   0/ 38] 	 train loss: 2.045128 	 lr: 0.01000
[epoch   5:   0/ 38] 	 train loss: 1.713719 	 lr: 0.01000
[epoch   6:   0/ 38] 	 train loss: 1.601057 	 lr: 0.01000
[epoch   7:   0/ 38] 	 train loss: 1.545464 	 lr: 0.01000
[epoch   8:   0/ 38] 	 train loss: 1.550001 	 lr: 0.01000
[epoch   9:   0/ 38] 	 train loss: 1.279726 	 lr: 0.01000
[epoch  10:   0/ 38] 	 train loss: 1.474987 	 lr: 0.01000
now evaluate...

val loss: 1.687903 	 acc: 0.487844

[epoch  11:   0/ 38] 	 train loss: 1.283477 	 lr: 0.01000
[epoch  12:   0/ 38] 	 train loss: 1.239531 	 lr: 0.01000
[epoch  13:   0/ 38] 	 train loss: 1.319904 	 lr: 0.01000
[epoch  14:   0/ 38] 	 train loss: 1.119637 	 lr: 0.01000
[epoch  15:   0/ 38] 	 train loss: 1.237361 	 lr: 0.01000
[epoch  16:   0/ 38] 	 train loss: 1.259042 	 lr: 0.01000
[epoch  17:   0/ 38] 	 train loss: 1.125373 	 lr: 0.01000
[epoch  18:   0/ 38] 	 train loss: 1.166342 	 lr: 0.01000
[epoch  19:   0/ 38] 	 train loss: 1.224637 	 lr: 0.01000
[epoch  20:   0/ 38] 	 train loss: 1.161625 	 lr: 0.01000
now evaluate...

val loss: 1.616997 	 acc: 0.520259

[epoch  21:   0/ 38] 	 train loss: 1.174961 	 lr: 0.01000
[epoch  22:   0/ 38] 	 train loss: 1.043407 	 lr: 0.01000
[epoch  23:   0/ 38] 	 train loss: 1.065572 	 lr: 0.01000
[epoch  24:   0/ 38] 	 train loss: 1.010356 	 lr: 0.01000
[epoch  25:   0/ 38] 	 train loss: 1.033765 	 lr: 0.01000
[epoch  26:   0/ 38] 	 train loss: 1.159443 	 lr: 0.01000
[epoch  27:   0/ 38] 	 train loss: 1.045442 	 lr: 0.01000
[epoch  28:   0/ 38] 	 train loss: 1.033762 	 lr: 0.01000
[epoch  29:   0/ 38] 	 train loss: 1.174126 	 lr: 0.01000
[epoch  30:   0/ 38] 	 train loss: 1.035617 	 lr: 0.01000
now evaluate...

val loss: 1.274344 	 acc: 0.617909

[epoch  31:   0/ 38] 	 train loss: 1.173964 	 lr: 0.01000
[epoch  32:   0/ 38] 	 train loss: 1.081507 	 lr: 0.01000
[epoch  33:   0/ 38] 	 train loss: 1.075313 	 lr: 0.01000
[epoch  34:   0/ 38] 	 train loss: 1.024875 	 lr: 0.01000
[epoch  35:   0/ 38] 	 train loss: 0.959379 	 lr: 0.01000
[epoch  36:   0/ 38] 	 train loss: 1.009696 	 lr: 0.01000
[epoch  37:   0/ 38] 	 train loss: 1.133743 	 lr: 0.01000
[epoch  38:   0/ 38] 	 train loss: 1.025180 	 lr: 0.01000
[epoch  39:   0/ 38] 	 train loss: 1.041131 	 lr: 0.01000
[epoch  40:   0/ 38] 	 train loss: 1.060500 	 lr: 0.01000
now evaluate...

val loss: 1.397509 	 acc: 0.561588

[epoch  41:   0/ 38] 	 train loss: 1.036127 	 lr: 0.01000
[epoch  42:   0/ 38] 	 train loss: 1.168808 	 lr: 0.01000
[epoch  43:   0/ 38] 	 train loss: 1.126974 	 lr: 0.01000
[epoch  44:   0/ 38] 	 train loss: 1.076505 	 lr: 0.01000
[epoch  45:   0/ 38] 	 train loss: 1.073167 	 lr: 0.01000
[epoch  46:   0/ 38] 	 train loss: 0.967808 	 lr: 0.01000
[epoch  47:   0/ 38] 	 train loss: 1.057150 	 lr: 0.01000
[epoch  48:   0/ 38] 	 train loss: 0.933059 	 lr: 0.01000
[epoch  49:   0/ 38] 	 train loss: 0.891079 	 lr: 0.01000
[epoch  50:   0/ 38] 	 train loss: 0.866997 	 lr: 0.01000
now evaluate...

val loss: 1.373123 	 acc: 0.570097

[epoch  51:   0/ 38] 	 train loss: 0.944491 	 lr: 0.01000
[epoch  52:   0/ 38] 	 train loss: 0.998380 	 lr: 0.00700
[epoch  53:   0/ 38] 	 train loss: 0.856223 	 lr: 0.00700
[epoch  54:   0/ 38] 	 train loss: 0.925806 	 lr: 0.00700
[epoch  55:   0/ 38] 	 train loss: 0.916314 	 lr: 0.00700
[epoch  56:   0/ 38] 	 train loss: 0.810338 	 lr: 0.00700
[epoch  57:   0/ 38] 	 train loss: 1.013742 	 lr: 0.00700
[epoch  58:   0/ 38] 	 train loss: 0.891941 	 lr: 0.00700
[epoch  59:   0/ 38] 	 train loss: 0.947585 	 lr: 0.00700
[epoch  60:   0/ 38] 	 train loss: 0.916359 	 lr: 0.00700
now evaluate...

val loss: 1.168023 	 acc: 0.636143

[epoch  61:   0/ 38] 	 train loss: 0.858803 	 lr: 0.00700
[epoch  62:   0/ 38] 	 train loss: 0.877996 	 lr: 0.00700
[epoch  63:   0/ 38] 	 train loss: 1.001239 	 lr: 0.00700
[epoch  64:   0/ 38] 	 train loss: 0.872642 	 lr: 0.00700
[epoch  65:   0/ 38] 	 train loss: 0.945773 	 lr: 0.00700
[epoch  66:   0/ 38] 	 train loss: 0.924273 	 lr: 0.00700
[epoch  67:   0/ 38] 	 train loss: 0.918208 	 lr: 0.00700
[epoch  68:   0/ 38] 	 train loss: 0.803794 	 lr: 0.00700
[epoch  69:   0/ 38] 	 train loss: 0.908807 	 lr: 0.00700
now evaluate...

val loss: 1.205089 	 acc: 0.624797

[epoch  70:   0/ 38] 	 train loss: 0.909179 	 lr: 0.00700
[epoch  71:   0/ 38] 	 train loss: 1.033339 	 lr: 0.00700
[epoch  72:   0/ 38] 	 train loss: 0.845611 	 lr: 0.00700
[epoch  73:   0/ 38] 	 train loss: 0.922917 	 lr: 0.00700
[epoch  74:   0/ 38] 	 train loss: 0.999551 	 lr: 0.00700
[epoch  75:   0/ 38] 	 train loss: 0.943321 	 lr: 0.00700
[epoch  76:   0/ 38] 	 train loss: 0.960763 	 lr: 0.00700
[epoch  77:   0/ 38] 	 train loss: 0.805410 	 lr: 0.00700
[epoch  78:   0/ 38] 	 train loss: 0.916087 	 lr: 0.00700
[epoch  79:   0/ 38] 	 train loss: 0.849330 	 lr: 0.00700
now evaluate...

val loss: 1.176273 	 acc: 0.631686

[epoch  80:   0/ 38] 	 train loss: 0.860143 	 lr: 0.00700
[epoch  81:   0/ 38] 	 train loss: 0.779201 	 lr: 0.00700
[epoch  82:   0/ 38] 	 train loss: 0.786908 	 lr: 0.00700
[epoch  83:   0/ 38] 	 train loss: 0.918718 	 lr: 0.00700
[epoch  84:   0/ 38] 	 train loss: 0.872002 	 lr: 0.00700
[epoch  85:   0/ 38] 	 train loss: 0.908233 	 lr: 0.00700
[epoch  86:   0/ 38] 	 train loss: 0.990322 	 lr: 0.00700
[epoch  87:   0/ 38] 	 train loss: 0.850520 	 lr: 0.00700
[epoch  88:   0/ 38] 	 train loss: 0.804549 	 lr: 0.00700
[epoch  89:   0/ 38] 	 train loss: 0.851893 	 lr: 0.00700
now evaluate...

val loss: 1.363296 	 acc: 0.579417

[epoch  90:   0/ 38] 	 train loss: 0.804947 	 lr: 0.00700
[epoch  91:   0/ 38] 	 train loss: 0.840180 	 lr: 0.00700
[epoch  92:   0/ 38] 	 train loss: 0.912790 	 lr: 0.00700
[epoch  93:   0/ 38] 	 train loss: 0.811017 	 lr: 0.00700
[epoch  94:   0/ 38] 	 train loss: 0.923420 	 lr: 0.00700
[epoch  95:   0/ 38] 	 train loss: 0.872256 	 lr: 0.00700
[epoch  96:   0/ 38] 	 train loss: 0.813162 	 lr: 0.00700
[epoch  97:   0/ 38] 	 train loss: 0.867481 	 lr: 0.00700
[epoch  98:   0/ 38] 	 train loss: 0.865157 	 lr: 0.00700
[epoch  99:   0/ 38] 	 train loss: 0.796897 	 lr: 0.00700
now evaluate...

val loss: 1.251996 	 acc: 0.616288

[epoch 100:   0/ 38] 	 train loss: 0.681873 	 lr: 0.00700
[epoch 101:   0/ 38] 	 train loss: 0.756182 	 lr: 0.00700
[epoch 102:   0/ 38] 	 train loss: 0.805490 	 lr: 0.00490
[epoch 103:   0/ 38] 	 train loss: 0.766610 	 lr: 0.00490
[epoch 104:   0/ 38] 	 train loss: 0.876667 	 lr: 0.00490
[epoch 105:   0/ 38] 	 train loss: 0.752567 	 lr: 0.00490
[epoch 106:   0/ 38] 	 train loss: 0.768588 	 lr: 0.00490
[epoch 107:   0/ 38] 	 train loss: 0.931796 	 lr: 0.00490
[epoch 108:   0/ 38] 	 train loss: 0.793208 	 lr: 0.00490
[epoch 109:   0/ 38] 	 train loss: 0.770150 	 lr: 0.00490
now evaluate...

val loss: 1.157700 	 acc: 0.637358

[epoch 110:   0/ 38] 	 train loss: 0.911311 	 lr: 0.00490
[epoch 111:   0/ 38] 	 train loss: 0.728427 	 lr: 0.00490
[epoch 112:   0/ 38] 	 train loss: 0.777510 	 lr: 0.00490
[epoch 113:   0/ 38] 	 train loss: 0.883246 	 lr: 0.00490
[epoch 114:   0/ 38] 	 train loss: 0.847988 	 lr: 0.00490
[epoch 115:   0/ 38] 	 train loss: 0.718853 	 lr: 0.00490
[epoch 116:   0/ 38] 	 train loss: 0.800850 	 lr: 0.00490
[epoch 117:   0/ 38] 	 train loss: 0.781339 	 lr: 0.00490
[epoch 118:   0/ 38] 	 train loss: 0.734433 	 lr: 0.00490
[epoch 119:   0/ 38] 	 train loss: 0.814707 	 lr: 0.00490
now evaluate...

val loss: 1.060273 	 acc: 0.670583

[epoch 120:   0/ 38] 	 train loss: 0.775362 	 lr: 0.00490
[epoch 121:   0/ 38] 	 train loss: 0.778117 	 lr: 0.00490
[epoch 122:   0/ 38] 	 train loss: 0.803519 	 lr: 0.00490
[epoch 123:   0/ 38] 	 train loss: 0.687602 	 lr: 0.00490
[epoch 124:   0/ 38] 	 train loss: 0.802409 	 lr: 0.00490
[epoch 125:   0/ 38] 	 train loss: 0.817430 	 lr: 0.00490
[epoch 126:   0/ 38] 	 train loss: 0.811972 	 lr: 0.00490
[epoch 127:   0/ 38] 	 train loss: 0.837995 	 lr: 0.00490
[epoch 128:   0/ 38] 	 train loss: 0.757958 	 lr: 0.00490
now evaluate...

val loss: 1.103093 	 acc: 0.664100

[epoch 129:   0/ 38] 	 train loss: 0.805335 	 lr: 0.00490
[epoch 130:   0/ 38] 	 train loss: 0.791834 	 lr: 0.00490
[epoch 131:   0/ 38] 	 train loss: 0.816846 	 lr: 0.00490
[epoch 132:   0/ 38] 	 train loss: 0.688557 	 lr: 0.00490
[epoch 133:   0/ 38] 	 train loss: 0.727238 	 lr: 0.00490
[epoch 134:   0/ 38] 	 train loss: 0.908019 	 lr: 0.00490
[epoch 135:   0/ 38] 	 train loss: 0.640145 	 lr: 0.00490
[epoch 136:   0/ 38] 	 train loss: 0.908850 	 lr: 0.00490
[epoch 137:   0/ 38] 	 train loss: 0.730467 	 lr: 0.00490
[epoch 138:   0/ 38] 	 train loss: 0.767426 	 lr: 0.00490
now evaluate...

val loss: 1.048835 	 acc: 0.681524

[epoch 139:   0/ 38] 	 train loss: 0.924986 	 lr: 0.00490
[epoch 140:   0/ 38] 	 train loss: 0.725798 	 lr: 0.00490
[epoch 141:   0/ 38] 	 train loss: 0.784762 	 lr: 0.00490
[epoch 142:   0/ 38] 	 train loss: 0.816611 	 lr: 0.00490
[epoch 143:   0/ 38] 	 train loss: 0.820029 	 lr: 0.00490
[epoch 144:   0/ 38] 	 train loss: 0.842703 	 lr: 0.00490
[epoch 145:   0/ 38] 	 train loss: 0.723705 	 lr: 0.00490
[epoch 146:   0/ 38] 	 train loss: 0.726777 	 lr: 0.00490
[epoch 147:   0/ 38] 	 train loss: 0.768357 	 lr: 0.00490
[epoch 148:   0/ 38] 	 train loss: 0.873430 	 lr: 0.00490
now evaluate...

val loss: 0.976044 	 acc: 0.688817

[epoch 149:   0/ 38] 	 train loss: 0.843503 	 lr: 0.00490
[epoch 150:   0/ 38] 	 train loss: 0.705631 	 lr: 0.00490
[epoch 151:   0/ 38] 	 train loss: 0.848252 	 lr: 0.00490
[epoch 152:   0/ 38] 	 train loss: 0.984167 	 lr: 0.00343
[epoch 153:   0/ 38] 	 train loss: 0.699522 	 lr: 0.00343
[epoch 154:   0/ 38] 	 train loss: 0.631997 	 lr: 0.00343
[epoch 155:   0/ 38] 	 train loss: 0.622287 	 lr: 0.00343
[epoch 156:   0/ 38] 	 train loss: 0.876400 	 lr: 0.00343
[epoch 157:   0/ 38] 	 train loss: 0.799198 	 lr: 0.00343
[epoch 158:   0/ 38] 	 train loss: 0.809579 	 lr: 0.00343
now evaluate...

val loss: 1.052254 	 acc: 0.659238

[epoch 159:   0/ 38] 	 train loss: 0.706703 	 lr: 0.00343
[epoch 160:   0/ 38] 	 train loss: 0.683986 	 lr: 0.00343
[epoch 161:   0/ 38] 	 train loss: 0.806056 	 lr: 0.00343
[epoch 162:   0/ 38] 	 train loss: 0.705160 	 lr: 0.00343
[epoch 163:   0/ 38] 	 train loss: 0.757126 	 lr: 0.00343
[epoch 164:   0/ 38] 	 train loss: 0.684482 	 lr: 0.00343
[epoch 165:   0/ 38] 	 train loss: 0.635888 	 lr: 0.00343
[epoch 166:   0/ 38] 	 train loss: 0.752471 	 lr: 0.00343
[epoch 167:   0/ 38] 	 train loss: 0.709198 	 lr: 0.00343
[epoch 168:   0/ 38] 	 train loss: 0.658481 	 lr: 0.00343
now evaluate...

val loss: 0.997143 	 acc: 0.691653

[epoch 169:   0/ 38] 	 train loss: 0.742640 	 lr: 0.00343
[epoch 170:   0/ 38] 	 train loss: 0.618097 	 lr: 0.00343
[epoch 171:   0/ 38] 	 train loss: 0.745490 	 lr: 0.00343
[epoch 172:   0/ 38] 	 train loss: 0.676919 	 lr: 0.00343
[epoch 173:   0/ 38] 	 train loss: 0.764851 	 lr: 0.00343
[epoch 174:   0/ 38] 	 train loss: 0.646721 	 lr: 0.00343
[epoch 175:   0/ 38] 	 train loss: 0.758541 	 lr: 0.00343
[epoch 176:   0/ 38] 	 train loss: 0.774243 	 lr: 0.00343
[epoch 177:   0/ 38] 	 train loss: 0.745298 	 lr: 0.00343
[epoch 178:   0/ 38] 	 train loss: 0.767502 	 lr: 0.00343
now evaluate...

val loss: 0.978504 	 acc: 0.697326

[epoch 179:   0/ 38] 	 train loss: 0.593080 	 lr: 0.00343
[epoch 180:   0/ 38] 	 train loss: 0.819218 	 lr: 0.00343
[epoch 181:   0/ 38] 	 train loss: 0.555701 	 lr: 0.00343
[epoch 182:   0/ 38] 	 train loss: 0.670557 	 lr: 0.00343
[epoch 183:   0/ 38] 	 train loss: 0.784184 	 lr: 0.00343
[epoch 184:   0/ 38] 	 train loss: 0.637688 	 lr: 0.00343
[epoch 185:   0/ 38] 	 train loss: 0.648850 	 lr: 0.00343
[epoch 186:   0/ 38] 	 train loss: 0.856154 	 lr: 0.00343
[epoch 187:   0/ 38] 	 train loss: 0.719791 	 lr: 0.00343
[epoch 188:   0/ 38] 	 train loss: 0.595918 	 lr: 0.00343
now evaluate...

val loss: 1.038836 	 acc: 0.682334

[epoch 189:   0/ 38] 	 train loss: 0.660262 	 lr: 0.00343
[epoch 190:   0/ 38] 	 train loss: 0.669435 	 lr: 0.00343
[epoch 191:   0/ 38] 	 train loss: 0.638591 	 lr: 0.00343
[epoch 192:   0/ 38] 	 train loss: 0.652104 	 lr: 0.00343
[epoch 193:   0/ 38] 	 train loss: 0.693068 	 lr: 0.00343
[epoch 194:   0/ 38] 	 train loss: 0.628172 	 lr: 0.00343
[epoch 195:   0/ 38] 	 train loss: 0.722861 	 lr: 0.00343
[epoch 196:   0/ 38] 	 train loss: 0.672675 	 lr: 0.00343
[epoch 197:   0/ 38] 	 train loss: 0.651453 	 lr: 0.00343
now evaluate...

val loss: 1.053151 	 acc: 0.681524

[epoch 198:   0/ 38] 	 train loss: 0.792765 	 lr: 0.00343
[epoch 199:   0/ 38] 	 train loss: 0.740652 	 lr: 0.00343
[epoch 200:   0/ 38] 	 train loss: 0.741318 	 lr: 0.00343
[epoch 201:   0/ 38] 	 train loss: 0.675544 	 lr: 0.00343
[epoch 202:   0/ 38] 	 train loss: 0.877944 	 lr: 0.00240
[epoch 203:   0/ 38] 	 train loss: 0.657577 	 lr: 0.00240
[epoch 204:   0/ 38] 	 train loss: 0.725901 	 lr: 0.00240
[epoch 205:   0/ 38] 	 train loss: 0.611189 	 lr: 0.00240
[epoch 206:   0/ 38] 	 train loss: 0.616389 	 lr: 0.00240
[epoch 207:   0/ 38] 	 train loss: 0.645836 	 lr: 0.00240
now evaluate...

val loss: 0.996779 	 acc: 0.696921

[epoch 208:   0/ 38] 	 train loss: 0.523920 	 lr: 0.00240
[epoch 209:   0/ 38] 	 train loss: 0.580874 	 lr: 0.00240
[epoch 210:   0/ 38] 	 train loss: 0.733768 	 lr: 0.00240
[epoch 211:   0/ 38] 	 train loss: 0.710112 	 lr: 0.00240
[epoch 212:   0/ 38] 	 train loss: 0.691950 	 lr: 0.00240
[epoch 213:   0/ 38] 	 train loss: 0.797787 	 lr: 0.00240
[epoch 214:   0/ 38] 	 train loss: 0.656141 	 lr: 0.00240
[epoch 215:   0/ 38] 	 train loss: 0.522031 	 lr: 0.00240
[epoch 216:   0/ 38] 	 train loss: 0.550177 	 lr: 0.00240
[epoch 217:   0/ 38] 	 train loss: 0.589479 	 lr: 0.00240
now evaluate...

val loss: 1.015721 	 acc: 0.692464

[epoch 218:   0/ 38] 	 train loss: 0.802206 	 lr: 0.00240
[epoch 219:   0/ 38] 	 train loss: 0.707384 	 lr: 0.00240
[epoch 220:   0/ 38] 	 train loss: 0.681927 	 lr: 0.00240
[epoch 221:   0/ 38] 	 train loss: 0.614061 	 lr: 0.00240
[epoch 222:   0/ 38] 	 train loss: 0.736907 	 lr: 0.00240
[epoch 223:   0/ 38] 	 train loss: 0.645017 	 lr: 0.00240
[epoch 224:   0/ 38] 	 train loss: 0.707883 	 lr: 0.00240
[epoch 225:   0/ 38] 	 train loss: 0.574134 	 lr: 0.00240
[epoch 226:   0/ 38] 	 train loss: 0.739548 	 lr: 0.00240
[epoch 227:   0/ 38] 	 train loss: 0.711732 	 lr: 0.00240
now evaluate...

val loss: 0.955200 	 acc: 0.711102

[epoch 228:   0/ 38] 	 train loss: 0.746154 	 lr: 0.00240
[epoch 229:   0/ 38] 	 train loss: 0.666009 	 lr: 0.00240
[epoch 230:   0/ 38] 	 train loss: 0.660141 	 lr: 0.00240
[epoch 231:   0/ 38] 	 train loss: 0.597369 	 lr: 0.00240
[epoch 232:   0/ 38] 	 train loss: 0.693358 	 lr: 0.00240
[epoch 233:   0/ 38] 	 train loss: 0.586757 	 lr: 0.00240
[epoch 234:   0/ 38] 	 train loss: 0.635601 	 lr: 0.00240
[epoch 235:   0/ 38] 	 train loss: 0.773145 	 lr: 0.00240
[epoch 236:   0/ 38] 	 train loss: 0.552451 	 lr: 0.00240
[epoch 237:   0/ 38] 	 train loss: 0.644639 	 lr: 0.00240
now evaluate...

val loss: 0.973807 	 acc: 0.702593

[epoch 238:   0/ 38] 	 train loss: 0.723473 	 lr: 0.00240
[epoch 239:   0/ 38] 	 train loss: 0.734180 	 lr: 0.00240
[epoch 240:   0/ 38] 	 train loss: 0.525398 	 lr: 0.00240
[epoch 241:   0/ 38] 	 train loss: 0.600687 	 lr: 0.00240
[epoch 242:   0/ 38] 	 train loss: 0.572376 	 lr: 0.00240
[epoch 243:   0/ 38] 	 train loss: 0.571998 	 lr: 0.00240
[epoch 244:   0/ 38] 	 train loss: 0.731938 	 lr: 0.00240
[epoch 245:   0/ 38] 	 train loss: 0.643910 	 lr: 0.00240
[epoch 246:   0/ 38] 	 train loss: 0.593108 	 lr: 0.00240
[epoch 247:   0/ 38] 	 train loss: 0.636898 	 lr: 0.00240
now evaluate...

val loss: 1.001454 	 acc: 0.696110

[epoch 248:   0/ 38] 	 train loss: 0.584911 	 lr: 0.00240
[epoch 249:   0/ 38] 	 train loss: 0.551577 	 lr: 0.00240
[epoch 250:   0/ 38] 	 train loss: 0.788760 	 lr: 0.00240
[epoch 251:   0/ 38] 	 train loss: 0.625914 	 lr: 0.00240
[epoch 252:   0/ 38] 	 train loss: 0.708475 	 lr: 0.00168
[epoch 253:   0/ 38] 	 train loss: 0.640636 	 lr: 0.00168
[epoch 254:   0/ 38] 	 train loss: 0.570496 	 lr: 0.00168
[epoch 255:   0/ 38] 	 train loss: 0.610522 	 lr: 0.00168
[epoch 256:   0/ 38] 	 train loss: 0.710071 	 lr: 0.00168
now evaluate...

val loss: 0.979434 	 acc: 0.701378

[epoch 257:   0/ 38] 	 train loss: 0.667144 	 lr: 0.00168
[epoch 258:   0/ 38] 	 train loss: 0.691715 	 lr: 0.00168
[epoch 259:   0/ 38] 	 train loss: 0.677504 	 lr: 0.00168
[epoch 260:   0/ 38] 	 train loss: 0.621259 	 lr: 0.00168
[epoch 261:   0/ 38] 	 train loss: 0.634698 	 lr: 0.00168
[epoch 262:   0/ 38] 	 train loss: 0.719927 	 lr: 0.00168
[epoch 263:   0/ 38] 	 train loss: 0.557302 	 lr: 0.00168
[epoch 264:   0/ 38] 	 train loss: 0.591191 	 lr: 0.00168
[epoch 265:   0/ 38] 	 train loss: 0.596732 	 lr: 0.00168
[epoch 266:   0/ 38] 	 train loss: 0.632301 	 lr: 0.00168
now evaluate...

val loss: 0.958664 	 acc: 0.703404

[epoch 267:   0/ 38] 	 train loss: 0.453780 	 lr: 0.00168
[epoch 268:   0/ 38] 	 train loss: 0.658543 	 lr: 0.00168
[epoch 269:   0/ 38] 	 train loss: 0.730350 	 lr: 0.00168
[epoch 270:   0/ 38] 	 train loss: 0.539527 	 lr: 0.00168
[epoch 271:   0/ 38] 	 train loss: 0.595237 	 lr: 0.00168
[epoch 272:   0/ 38] 	 train loss: 0.645680 	 lr: 0.00168
[epoch 273:   0/ 38] 	 train loss: 0.727949 	 lr: 0.00168
[epoch 274:   0/ 38] 	 train loss: 0.597921 	 lr: 0.00168
[epoch 275:   0/ 38] 	 train loss: 0.658673 	 lr: 0.00168
[epoch 276:   0/ 38] 	 train loss: 0.501149 	 lr: 0.00168
now evaluate...

val loss: 0.926112 	 acc: 0.714749

[epoch 277:   0/ 38] 	 train loss: 0.493965 	 lr: 0.00168
[epoch 278:   0/ 38] 	 train loss: 0.716333 	 lr: 0.00168
[epoch 279:   0/ 38] 	 train loss: 0.685548 	 lr: 0.00168
[epoch 280:   0/ 38] 	 train loss: 0.565815 	 lr: 0.00168
[epoch 281:   0/ 38] 	 train loss: 0.631987 	 lr: 0.00168
[epoch 282:   0/ 38] 	 train loss: 0.635619 	 lr: 0.00168
[epoch 283:   0/ 38] 	 train loss: 0.491478 	 lr: 0.00168
[epoch 284:   0/ 38] 	 train loss: 0.531752 	 lr: 0.00168
[epoch 285:   0/ 38] 	 train loss: 0.606850 	 lr: 0.00168
[epoch 286:   0/ 38] 	 train loss: 0.586410 	 lr: 0.00168
now evaluate...

val loss: 0.937174 	 acc: 0.724473

[epoch 287:   0/ 38] 	 train loss: 0.579156 	 lr: 0.00168
[epoch 288:   0/ 38] 	 train loss: 0.534377 	 lr: 0.00168
[epoch 289:   0/ 38] 	 train loss: 0.507446 	 lr: 0.00168
[epoch 290:   0/ 38] 	 train loss: 0.654284 	 lr: 0.00168
[epoch 291:   0/ 38] 	 train loss: 0.673978 	 lr: 0.00168
[epoch 292:   0/ 38] 	 train loss: 0.568080 	 lr: 0.00168
[epoch 293:   0/ 38] 	 train loss: 0.696121 	 lr: 0.00168
[epoch 294:   0/ 38] 	 train loss: 0.628019 	 lr: 0.00168
[epoch 295:   0/ 38] 	 train loss: 0.674840 	 lr: 0.00168
[epoch 296:   0/ 38] 	 train loss: 0.638872 	 lr: 0.00168
now evaluate...

val loss: 0.980407 	 acc: 0.705024

[epoch 297:   0/ 38] 	 train loss: 0.653064 	 lr: 0.00168
[epoch 298:   0/ 38] 	 train loss: 0.557955 	 lr: 0.00168
[epoch 299:   0/ 38] 	 train loss: 0.627649 	 lr: 0.00168
[epoch 300:   0/ 38] 	 train loss: 0.591230 	 lr: 0.00168
[epoch 301:   0/ 38] 	 train loss: 0.598193 	 lr: 0.00168
[epoch 302:   0/ 38] 	 train loss: 0.594675 	 lr: 0.00118
[epoch 303:   0/ 38] 	 train loss: 0.559620 	 lr: 0.00118
[epoch 304:   0/ 38] 	 train loss: 0.502038 	 lr: 0.00118
[epoch 305:   0/ 38] 	 train loss: 0.563957 	 lr: 0.00118
[epoch 306:   0/ 38] 	 train loss: 0.580221 	 lr: 0.00118
now evaluate...

val loss: 0.917249 	 acc: 0.717585

[epoch 307:   0/ 38] 	 train loss: 0.592146 	 lr: 0.00118
[epoch 308:   0/ 38] 	 train loss: 0.733216 	 lr: 0.00118
[epoch 309:   0/ 38] 	 train loss: 0.538594 	 lr: 0.00118
[epoch 310:   0/ 38] 	 train loss: 0.578501 	 lr: 0.00118
[epoch 311:   0/ 38] 	 train loss: 0.642237 	 lr: 0.00118
[epoch 312:   0/ 38] 	 train loss: 0.507860 	 lr: 0.00118
[epoch 313:   0/ 38] 	 train loss: 0.736841 	 lr: 0.00118
[epoch 314:   0/ 38] 	 train loss: 0.680317 	 lr: 0.00118
[epoch 315:   0/ 38] 	 train loss: 0.701489 	 lr: 0.00118
[epoch 316:   0/ 38] 	 train loss: 0.618501 	 lr: 0.00118
now evaluate...

val loss: 0.951043 	 acc: 0.713533

[epoch 317:   0/ 38] 	 train loss: 0.534846 	 lr: 0.00118
[epoch 318:   0/ 38] 	 train loss: 0.591483 	 lr: 0.00118
[epoch 319:   0/ 38] 	 train loss: 0.577950 	 lr: 0.00118
[epoch 320:   0/ 38] 	 train loss: 0.570401 	 lr: 0.00118
[epoch 321:   0/ 38] 	 train loss: 0.694145 	 lr: 0.00118
[epoch 322:   0/ 38] 	 train loss: 0.637536 	 lr: 0.00118
[epoch 323:   0/ 38] 	 train loss: 0.512178 	 lr: 0.00118
[epoch 324:   0/ 38] 	 train loss: 0.557313 	 lr: 0.00118
[epoch 325:   0/ 38] 	 train loss: 0.560790 	 lr: 0.00118
now evaluate...

val loss: 0.930773 	 acc: 0.723663

[epoch 326:   0/ 38] 	 train loss: 0.580532 	 lr: 0.00118
[epoch 327:   0/ 38] 	 train loss: 0.562232 	 lr: 0.00118
[epoch 328:   0/ 38] 	 train loss: 0.652244 	 lr: 0.00118
[epoch 329:   0/ 38] 	 train loss: 0.599930 	 lr: 0.00118
[epoch 330:   0/ 38] 	 train loss: 0.618169 	 lr: 0.00118
[epoch 331:   0/ 38] 	 train loss: 0.536515 	 lr: 0.00118
[epoch 332:   0/ 38] 	 train loss: 0.498554 	 lr: 0.00118
[epoch 333:   0/ 38] 	 train loss: 0.608410 	 lr: 0.00118
[epoch 334:   0/ 38] 	 train loss: 0.455600 	 lr: 0.00118
[epoch 335:   0/ 38] 	 train loss: 0.550517 	 lr: 0.00118
now evaluate...

val loss: 0.954826 	 acc: 0.708671

[epoch 336:   0/ 38] 	 train loss: 0.618965 	 lr: 0.00118
[epoch 337:   0/ 38] 	 train loss: 0.608966 	 lr: 0.00118
[epoch 338:   0/ 38] 	 train loss: 0.693498 	 lr: 0.00118
[epoch 339:   0/ 38] 	 train loss: 0.590462 	 lr: 0.00118
[epoch 340:   0/ 38] 	 train loss: 0.559958 	 lr: 0.00118
[epoch 341:   0/ 38] 	 train loss: 0.661314 	 lr: 0.00118
[epoch 342:   0/ 38] 	 train loss: 0.531328 	 lr: 0.00118
[epoch 343:   0/ 38] 	 train loss: 0.529713 	 lr: 0.00118
[epoch 344:   0/ 38] 	 train loss: 0.708168 	 lr: 0.00118
[epoch 345:   0/ 38] 	 train loss: 0.675761 	 lr: 0.00118
now evaluate...

val loss: 0.909464 	 acc: 0.720016

[epoch 346:   0/ 38] 	 train loss: 0.533652 	 lr: 0.00118
[epoch 347:   0/ 38] 	 train loss: 0.532702 	 lr: 0.00118
[epoch 348:   0/ 38] 	 train loss: 0.583545 	 lr: 0.00118
[epoch 349:   0/ 38] 	 train loss: 0.774103 	 lr: 0.00118
[epoch 350:   0/ 38] 	 train loss: 0.570270 	 lr: 0.00118
[epoch 351:   0/ 38] 	 train loss: 0.626800 	 lr: 0.00118
[epoch 352:   0/ 38] 	 train loss: 0.652394 	 lr: 0.00082
[epoch 353:   0/ 38] 	 train loss: 0.586993 	 lr: 0.00082
[epoch 354:   0/ 38] 	 train loss: 0.523562 	 lr: 0.00082
[epoch 355:   0/ 38] 	 train loss: 0.568534 	 lr: 0.00082
now evaluate...

val loss: 0.958288 	 acc: 0.709481

[epoch 356:   0/ 38] 	 train loss: 0.694528 	 lr: 0.00082
[epoch 357:   0/ 38] 	 train loss: 0.557904 	 lr: 0.00082
[epoch 358:   0/ 38] 	 train loss: 0.610706 	 lr: 0.00082
[epoch 359:   0/ 38] 	 train loss: 0.498605 	 lr: 0.00082
[epoch 360:   0/ 38] 	 train loss: 0.549823 	 lr: 0.00082
[epoch 361:   0/ 38] 	 train loss: 0.678722 	 lr: 0.00082
[epoch 362:   0/ 38] 	 train loss: 0.567321 	 lr: 0.00082
[epoch 363:   0/ 38] 	 train loss: 0.616035 	 lr: 0.00082
[epoch 364:   0/ 38] 	 train loss: 0.551669 	 lr: 0.00082
[epoch 365:   0/ 38] 	 train loss: 0.512372 	 lr: 0.00082
now evaluate...

val loss: 0.962497 	 acc: 0.707861

[epoch 366:   0/ 38] 	 train loss: 0.506101 	 lr: 0.00082
[epoch 367:   0/ 38] 	 train loss: 0.584019 	 lr: 0.00082
[epoch 368:   0/ 38] 	 train loss: 0.598852 	 lr: 0.00082
[epoch 369:   0/ 38] 	 train loss: 0.543524 	 lr: 0.00082
[epoch 370:   0/ 38] 	 train loss: 0.622074 	 lr: 0.00082
[epoch 371:   0/ 38] 	 train loss: 0.568805 	 lr: 0.00082
[epoch 372:   0/ 38] 	 train loss: 0.661839 	 lr: 0.00082
[epoch 373:   0/ 38] 	 train loss: 0.540229 	 lr: 0.00082
[epoch 374:   0/ 38] 	 train loss: 0.600990 	 lr: 0.00082
[epoch 375:   0/ 38] 	 train loss: 0.661930 	 lr: 0.00082
now evaluate...

val loss: 0.915802 	 acc: 0.712318

[epoch 376:   0/ 38] 	 train loss: 0.591771 	 lr: 0.00082
[epoch 377:   0/ 38] 	 train loss: 0.539839 	 lr: 0.00082
[epoch 378:   0/ 38] 	 train loss: 0.674717 	 lr: 0.00082
[epoch 379:   0/ 38] 	 train loss: 0.527582 	 lr: 0.00082
[epoch 380:   0/ 38] 	 train loss: 0.525033 	 lr: 0.00082
[epoch 381:   0/ 38] 	 train loss: 0.580997 	 lr: 0.00082
[epoch 382:   0/ 38] 	 train loss: 0.596658 	 lr: 0.00082
[epoch 383:   0/ 38] 	 train loss: 0.662340 	 lr: 0.00082
[epoch 384:   0/ 38] 	 train loss: 0.480245 	 lr: 0.00082
now evaluate...

val loss: 0.955612 	 acc: 0.717180

[epoch 385:   0/ 38] 	 train loss: 0.564163 	 lr: 0.00082
[epoch 386:   0/ 38] 	 train loss: 0.493546 	 lr: 0.00082
[epoch 387:   0/ 38] 	 train loss: 0.640241 	 lr: 0.00082
[epoch 388:   0/ 38] 	 train loss: 0.570839 	 lr: 0.00082
[epoch 389:   0/ 38] 	 train loss: 0.603157 	 lr: 0.00082
[epoch 390:   0/ 38] 	 train loss: 0.524968 	 lr: 0.00082
[epoch 391:   0/ 38] 	 train loss: 0.553213 	 lr: 0.00082
[epoch 392:   0/ 38] 	 train loss: 0.465340 	 lr: 0.00082
[epoch 393:   0/ 38] 	 train loss: 0.594818 	 lr: 0.00082
[epoch 394:   0/ 38] 	 train loss: 0.607916 	 lr: 0.00082
now evaluate...

val loss: 0.937114 	 acc: 0.707050

[epoch 395:   0/ 38] 	 train loss: 0.546866 	 lr: 0.00082
[epoch 396:   0/ 38] 	 train loss: 0.496808 	 lr: 0.00082
[epoch 397:   0/ 38] 	 train loss: 0.619677 	 lr: 0.00082
[epoch 398:   0/ 38] 	 train loss: 0.536251 	 lr: 0.00082
[epoch 399:   0/ 38] 	 train loss: 0.622312 	 lr: 0.00082
[epoch 400:   0/ 38] 	 train loss: 0.438441 	 lr: 0.00082
[epoch 401:   0/ 38] 	 train loss: 0.647844 	 lr: 0.00082
[epoch 402:   0/ 38] 	 train loss: 0.741008 	 lr: 0.00058
[epoch 403:   0/ 38] 	 train loss: 0.529541 	 lr: 0.00058
[epoch 404:   0/ 38] 	 train loss: 0.589386 	 lr: 0.00058
now evaluate...

val loss: 0.917006 	 acc: 0.728120

[epoch 405:   0/ 38] 	 train loss: 0.589849 	 lr: 0.00058
[epoch 406:   0/ 38] 	 train loss: 0.544709 	 lr: 0.00058
[epoch 407:   0/ 38] 	 train loss: 0.528617 	 lr: 0.00058
[epoch 408:   0/ 38] 	 train loss: 0.527373 	 lr: 0.00058
[epoch 409:   0/ 38] 	 train loss: 0.506774 	 lr: 0.00058
[epoch 410:   0/ 38] 	 train loss: 0.555094 	 lr: 0.00058
[epoch 411:   0/ 38] 	 train loss: 0.513937 	 lr: 0.00058
[epoch 412:   0/ 38] 	 train loss: 0.548070 	 lr: 0.00058
[epoch 413:   0/ 38] 	 train loss: 0.574281 	 lr: 0.00058
[epoch 414:   0/ 38] 	 train loss: 0.556516 	 lr: 0.00058
now evaluate...

val loss: 0.921470 	 acc: 0.723663

[epoch 415:   0/ 38] 	 train loss: 0.489397 	 lr: 0.00058
[epoch 416:   0/ 38] 	 train loss: 0.529585 	 lr: 0.00058
[epoch 417:   0/ 38] 	 train loss: 0.560365 	 lr: 0.00058
[epoch 418:   0/ 38] 	 train loss: 0.581468 	 lr: 0.00058
[epoch 419:   0/ 38] 	 train loss: 0.623733 	 lr: 0.00058
[epoch 420:   0/ 38] 	 train loss: 0.560339 	 lr: 0.00058
[epoch 421:   0/ 38] 	 train loss: 0.739721 	 lr: 0.00058
[epoch 422:   0/ 38] 	 train loss: 0.626766 	 lr: 0.00058
[epoch 423:   0/ 38] 	 train loss: 0.499054 	 lr: 0.00058
[epoch 424:   0/ 38] 	 train loss: 0.616424 	 lr: 0.00058
now evaluate...

val loss: 0.923595 	 acc: 0.722853

[epoch 425:   0/ 38] 	 train loss: 0.559818 	 lr: 0.00058
[epoch 426:   0/ 38] 	 train loss: 0.545078 	 lr: 0.00058
[epoch 427:   0/ 38] 	 train loss: 0.439011 	 lr: 0.00058
[epoch 428:   0/ 38] 	 train loss: 0.507688 	 lr: 0.00058
[epoch 429:   0/ 38] 	 train loss: 0.598908 	 lr: 0.00058
[epoch 430:   0/ 38] 	 train loss: 0.670777 	 lr: 0.00058
[epoch 431:   0/ 38] 	 train loss: 0.552553 	 lr: 0.00058
[epoch 432:   0/ 38] 	 train loss: 0.594047 	 lr: 0.00058
[epoch 433:   0/ 38] 	 train loss: 0.547594 	 lr: 0.00058
[epoch 434:   0/ 38] 	 train loss: 0.448682 	 lr: 0.00058
now evaluate...

val loss: 0.886711 	 acc: 0.740276

[epoch 435:   0/ 38] 	 train loss: 0.525056 	 lr: 0.00058
[epoch 436:   0/ 38] 	 train loss: 0.633056 	 lr: 0.00058
[epoch 437:   0/ 38] 	 train loss: 0.546113 	 lr: 0.00058
[epoch 438:   0/ 38] 	 train loss: 0.548144 	 lr: 0.00058
[epoch 439:   0/ 38] 	 train loss: 0.574365 	 lr: 0.00058
[epoch 440:   0/ 38] 	 train loss: 0.495695 	 lr: 0.00058
[epoch 441:   0/ 38] 	 train loss: 0.495864 	 lr: 0.00058
[epoch 442:   0/ 38] 	 train loss: 0.491926 	 lr: 0.00058
[epoch 443:   0/ 38] 	 train loss: 0.547701 	 lr: 0.00058
[epoch 444:   0/ 38] 	 train loss: 0.548752 	 lr: 0.00058
now evaluate...

val loss: 0.901185 	 acc: 0.737844

[epoch 445:   0/ 38] 	 train loss: 0.599466 	 lr: 0.00058
[epoch 446:   0/ 38] 	 train loss: 0.510892 	 lr: 0.00058
[epoch 447:   0/ 38] 	 train loss: 0.464883 	 lr: 0.00058
[epoch 448:   0/ 38] 	 train loss: 0.543243 	 lr: 0.00058
[epoch 449:   0/ 38] 	 train loss: 0.628997 	 lr: 0.00058
[epoch 450:   0/ 38] 	 train loss: 0.611829 	 lr: 0.00058
[epoch 451:   0/ 38] 	 train loss: 0.434475 	 lr: 0.00058
[epoch 452:   0/ 38] 	 train loss: 0.538521 	 lr: 0.00040
[epoch 453:   0/ 38] 	 train loss: 0.546827 	 lr: 0.00040
now evaluate...

val loss: 0.904190 	 acc: 0.735008

[epoch 454:   0/ 38] 	 train loss: 0.423624 	 lr: 0.00040
[epoch 455:   0/ 38] 	 train loss: 0.470807 	 lr: 0.00040
[epoch 456:   0/ 38] 	 train loss: 0.512177 	 lr: 0.00040
[epoch 457:   0/ 38] 	 train loss: 0.543741 	 lr: 0.00040
[epoch 458:   0/ 38] 	 train loss: 0.531802 	 lr: 0.00040
[epoch 459:   0/ 38] 	 train loss: 0.566315 	 lr: 0.00040
[epoch 460:   0/ 38] 	 train loss: 0.581033 	 lr: 0.00040
[epoch 461:   0/ 38] 	 train loss: 0.500119 	 lr: 0.00040
[epoch 462:   0/ 38] 	 train loss: 0.505115 	 lr: 0.00040
[epoch 463:   0/ 38] 	 train loss: 0.499396 	 lr: 0.00040
now evaluate...

val loss: 0.902063 	 acc: 0.735008

[epoch 464:   0/ 38] 	 train loss: 0.630852 	 lr: 0.00040
[epoch 465:   0/ 38] 	 train loss: 0.496672 	 lr: 0.00040
[epoch 466:   0/ 38] 	 train loss: 0.512879 	 lr: 0.00040
[epoch 467:   0/ 38] 	 train loss: 0.596003 	 lr: 0.00040
[epoch 468:   0/ 38] 	 train loss: 0.566193 	 lr: 0.00040
[epoch 469:   0/ 38] 	 train loss: 0.564472 	 lr: 0.00040
[epoch 470:   0/ 38] 	 train loss: 0.528686 	 lr: 0.00040
[epoch 471:   0/ 38] 	 train loss: 0.643345 	 lr: 0.00040
[epoch 472:   0/ 38] 	 train loss: 0.464566 	 lr: 0.00040
[epoch 473:   0/ 38] 	 train loss: 0.515157 	 lr: 0.00040
now evaluate...

val loss: 0.926191 	 acc: 0.713128

[epoch 474:   0/ 38] 	 train loss: 0.506044 	 lr: 0.00040
[epoch 475:   0/ 38] 	 train loss: 0.510635 	 lr: 0.00040
[epoch 476:   0/ 38] 	 train loss: 0.599905 	 lr: 0.00040
[epoch 477:   0/ 38] 	 train loss: 0.466553 	 lr: 0.00040
[epoch 478:   0/ 38] 	 train loss: 0.575439 	 lr: 0.00040
[epoch 479:   0/ 38] 	 train loss: 0.618520 	 lr: 0.00040
[epoch 480:   0/ 38] 	 train loss: 0.528348 	 lr: 0.00040
[epoch 481:   0/ 38] 	 train loss: 0.529698 	 lr: 0.00040
[epoch 482:   0/ 38] 	 train loss: 0.598121 	 lr: 0.00040
[epoch 483:   0/ 38] 	 train loss: 0.545323 	 lr: 0.00040
now evaluate...

val loss: 0.899976 	 acc: 0.724878

[epoch 484:   0/ 38] 	 train loss: 0.542932 	 lr: 0.00040
[epoch 485:   0/ 38] 	 train loss: 0.556083 	 lr: 0.00040
[epoch 486:   0/ 38] 	 train loss: 0.545182 	 lr: 0.00040
[epoch 487:   0/ 38] 	 train loss: 0.557446 	 lr: 0.00040
[epoch 488:   0/ 38] 	 train loss: 0.619270 	 lr: 0.00040
[epoch 489:   0/ 38] 	 train loss: 0.578387 	 lr: 0.00040
[epoch 490:   0/ 38] 	 train loss: 0.515417 	 lr: 0.00040
[epoch 491:   0/ 38] 	 train loss: 0.470103 	 lr: 0.00040
[epoch 492:   0/ 38] 	 train loss: 0.522323 	 lr: 0.00040
[epoch 493:   0/ 38] 	 train loss: 0.582822 	 lr: 0.00040
now evaluate...

val loss: 0.906819 	 acc: 0.720421

[epoch 494:   0/ 38] 	 train loss: 0.575709 	 lr: 0.00040
[epoch 495:   0/ 38] 	 train loss: 0.467022 	 lr: 0.00040
[epoch 496:   0/ 38] 	 train loss: 0.578832 	 lr: 0.00040
[epoch 497:   0/ 38] 	 train loss: 0.441063 	 lr: 0.00040
[epoch 498:   0/ 38] 	 train loss: 0.571360 	 lr: 0.00040
[epoch 499:   0/ 38] 	 train loss: 0.516581 	 lr: 0.00040
[epoch 500:   0/ 38] 	 train loss: 0.582658 	 lr: 0.00040
[epoch 501:   0/ 38] 	 train loss: 0.587289 	 lr: 0.00040
[epoch 502:   0/ 38] 	 train loss: 0.448635 	 lr: 0.00028
[epoch 503:   0/ 38] 	 train loss: 0.594830 	 lr: 0.00028
now evaluate...

val loss: 0.909115 	 acc: 0.722447

[epoch 504:   0/ 38] 	 train loss: 0.431507 	 lr: 0.00028
[epoch 505:   0/ 38] 	 train loss: 0.586381 	 lr: 0.00028
[epoch 506:   0/ 38] 	 train loss: 0.438128 	 lr: 0.00028
[epoch 507:   0/ 38] 	 train loss: 0.593766 	 lr: 0.00028
[epoch 508:   0/ 38] 	 train loss: 0.589926 	 lr: 0.00028
[epoch 509:   0/ 38] 	 train loss: 0.487658 	 lr: 0.00028
[epoch 510:   0/ 38] 	 train loss: 0.497697 	 lr: 0.00028
[epoch 511:   0/ 38] 	 train loss: 0.593652 	 lr: 0.00028
[epoch 512:   0/ 38] 	 train loss: 0.517940 	 lr: 0.00028
now evaluate...

val loss: 0.931385 	 acc: 0.713128

[epoch 513:   0/ 38] 	 train loss: 0.431977 	 lr: 0.00028
[epoch 514:   0/ 38] 	 train loss: 0.518965 	 lr: 0.00028
[epoch 515:   0/ 38] 	 train loss: 0.605127 	 lr: 0.00028
[epoch 516:   0/ 38] 	 train loss: 0.512782 	 lr: 0.00028
[epoch 517:   0/ 38] 	 train loss: 0.472823 	 lr: 0.00028
[epoch 518:   0/ 38] 	 train loss: 0.499690 	 lr: 0.00028
[epoch 519:   0/ 38] 	 train loss: 0.638417 	 lr: 0.00028
[epoch 520:   0/ 38] 	 train loss: 0.487725 	 lr: 0.00028
[epoch 521:   0/ 38] 	 train loss: 0.455974 	 lr: 0.00028
[epoch 522:   0/ 38] 	 train loss: 0.533189 	 lr: 0.00028
now evaluate...
